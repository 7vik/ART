{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/sky_workdir/src/art/unsloth/model.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth.models import FastLanguageModel  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit with actual GPU utilization = 61.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 8192. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 37.83 GB. Also swap space = 6 GB.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.5.mlp', 'model.layers.24.mlp', 'model.layers.44.mlp', 'model.layers.46.mlp', 'model.layers.23.self_attn'], 'llm_int8_threshold': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc6f1248bb34480a9e0f3c262bb1423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880403dc52ba476e8391f467282d1c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:40<00:00,  1.07it/s]\n",
      "Unsloth 2025.3.18 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n",
      "usage: ipykernel_launcher.py [-h] [--host HOST] [--port PORT]\n",
      "                             [--uvicorn-log-level {debug,info,warning,error,critical,trace}]\n",
      "                             [--allow-credentials]\n",
      "                             [--allowed-origins ALLOWED_ORIGINS]\n",
      "                             [--allowed-methods ALLOWED_METHODS]\n",
      "                             [--allowed-headers ALLOWED_HEADERS]\n",
      "                             [--api-key API_KEY]\n",
      "                             [--lora-modules LORA_MODULES [LORA_MODULES ...]]\n",
      "                             [--prompt-adapters PROMPT_ADAPTERS [PROMPT_ADAPTERS ...]]\n",
      "                             [--chat-template CHAT_TEMPLATE]\n",
      "                             [--chat-template-content-format {auto,string,openai}]\n",
      "                             [--response-role RESPONSE_ROLE]\n",
      "                             [--ssl-keyfile SSL_KEYFILE]\n",
      "                             [--ssl-certfile SSL_CERTFILE]\n",
      "                             [--ssl-ca-certs SSL_CA_CERTS]\n",
      "                             [--ssl-cert-reqs SSL_CERT_REQS]\n",
      "                             [--root-path ROOT_PATH] [--middleware MIDDLEWARE]\n",
      "                             [--return-tokens-as-token-ids]\n",
      "                             [--disable-frontend-multiprocessing]\n",
      "                             [--enable-request-id-headers]\n",
      "                             [--enable-auto-tool-choice] [--enable-reasoning]\n",
      "                             [--reasoning-parser {deepseek_r1}]\n",
      "                             [--tool-call-parser {granite-20b-fc,granite,hermes,internlm,jamba,llama3_json,mistral,pythonic} or name registered in --tool-parser-plugin]\n",
      "                             [--tool-parser-plugin TOOL_PARSER_PLUGIN]\n",
      "                             [--model MODEL]\n",
      "                             [--task {auto,generate,embedding,embed,classify,score,reward,transcription}]\n",
      "                             [--tokenizer TOKENIZER] [--skip-tokenizer-init]\n",
      "                             [--revision REVISION]\n",
      "                             [--code-revision CODE_REVISION]\n",
      "                             [--tokenizer-revision TOKENIZER_REVISION]\n",
      "                             [--tokenizer-mode {auto,slow,mistral,custom}]\n",
      "                             [--trust-remote-code]\n",
      "                             [--allowed-local-media-path ALLOWED_LOCAL_MEDIA_PATH]\n",
      "                             [--download-dir DOWNLOAD_DIR]\n",
      "                             [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral,runai_streamer}]\n",
      "                             [--config-format {auto,hf,mistral}]\n",
      "                             [--dtype {auto,half,float16,bfloat16,float,float32}]\n",
      "                             [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]\n",
      "                             [--max-model-len MAX_MODEL_LEN]\n",
      "                             [--guided-decoding-backend {outlines,lm-format-enforcer,xgrammar}]\n",
      "                             [--logits-processor-pattern LOGITS_PROCESSOR_PATTERN]\n",
      "                             [--model-impl {auto,vllm,transformers}]\n",
      "                             [--distributed-executor-backend {ray,mp,uni,external_launcher}]\n",
      "                             [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]\n",
      "                             [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n",
      "                             [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n",
      "                             [--ray-workers-use-nsight]\n",
      "                             [--block-size {8,16,32,64,128}]\n",
      "                             [--enable-prefix-caching | --no-enable-prefix-caching]\n",
      "                             [--disable-sliding-window]\n",
      "                             [--use-v2-block-manager]\n",
      "                             [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]\n",
      "                             [--seed SEED] [--swap-space SWAP_SPACE]\n",
      "                             [--cpu-offload-gb CPU_OFFLOAD_GB]\n",
      "                             [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n",
      "                             [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]\n",
      "                             [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n",
      "                             [--max-num-partial-prefills MAX_NUM_PARTIAL_PREFILLS]\n",
      "                             [--max-long-partial-prefills MAX_LONG_PARTIAL_PREFILLS]\n",
      "                             [--long-prefill-token-threshold LONG_PREFILL_TOKEN_THRESHOLD]\n",
      "                             [--max-num-seqs MAX_NUM_SEQS]\n",
      "                             [--max-logprobs MAX_LOGPROBS]\n",
      "                             [--disable-log-stats]\n",
      "                             [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n",
      "                             [--rope-scaling ROPE_SCALING]\n",
      "                             [--rope-theta ROPE_THETA]\n",
      "                             [--hf-overrides HF_OVERRIDES] [--enforce-eager]\n",
      "                             [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]\n",
      "                             [--disable-custom-all-reduce]\n",
      "                             [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n",
      "                             [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n",
      "                             [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n",
      "                             [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n",
      "                             [--mm-processor-kwargs MM_PROCESSOR_KWARGS]\n",
      "                             [--disable-mm-preprocessor-cache] [--enable-lora]\n",
      "                             [--enable-lora-bias] [--max-loras MAX_LORAS]\n",
      "                             [--max-lora-rank MAX_LORA_RANK]\n",
      "                             [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n",
      "                             [--lora-dtype {auto,float16,bfloat16}]\n",
      "                             [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]\n",
      "                             [--max-cpu-loras MAX_CPU_LORAS]\n",
      "                             [--fully-sharded-loras] [--enable-prompt-adapter]\n",
      "                             [--max-prompt-adapters MAX_PROMPT_ADAPTERS]\n",
      "                             [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n",
      "                             [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu,hpu}]\n",
      "                             [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n",
      "                             [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]\n",
      "                             [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n",
      "                             [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n",
      "                             [--speculative-model SPECULATIVE_MODEL]\n",
      "                             [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n",
      "                             [--num-speculative-tokens NUM_SPECULATIVE_TOKENS]\n",
      "                             [--speculative-disable-mqa-scorer]\n",
      "                             [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]\n",
      "                             [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]\n",
      "                             [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]\n",
      "                             [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]\n",
      "                             [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]\n",
      "                             [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]\n",
      "                             [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]\n",
      "                             [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]\n",
      "                             [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]\n",
      "                             [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n",
      "                             [--ignore-patterns IGNORE_PATTERNS]\n",
      "                             [--preemption-mode PREEMPTION_MODE]\n",
      "                             [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n",
      "                             [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n",
      "                             [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n",
      "                             [--collect-detailed-traces COLLECT_DETAILED_TRACES]\n",
      "                             [--disable-async-output-proc]\n",
      "                             [--scheduling-policy {fcfs,priority}]\n",
      "                             [--scheduler-cls SCHEDULER_CLS]\n",
      "                             [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n",
      "                             [--override-pooler-config OVERRIDE_POOLER_CONFIG]\n",
      "                             [--compilation-config COMPILATION_CONFIG]\n",
      "                             [--kv-transfer-config KV_TRANSFER_CONFIG]\n",
      "                             [--worker-cls WORKER_CLS]\n",
      "                             [--generation-config GENERATION_CONFIG]\n",
      "                             [--override-generation-config OVERRIDE_GENERATION_CONFIG]\n",
      "                             [--enable-sleep-mode] [--calculate-kv-scales]\n",
      "                             [--additional-config ADDITIONAL_CONFIG]\n",
      "                             [--disable-log-requests]\n",
      "                             [--max-log-len MAX_LOG_LEN]\n",
      "                             [--disable-fastapi-docs]\n",
      "                             [--enable-prompt-tokens-details]\n",
      "ipykernel_launcher.py: error: argument --skip-tokenizer-init: ignored explicit argument 'False'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "api = art.UnslothAPI(in_process=True, wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api.get_or_create_model(\n",
    "    name=\"yes-or-no-unsloth-001\",\n",
    "    base_model=\"unsloth/Qwen2.5-14B-Instruct\",\n",
    ")\n",
    "\n",
    "\n",
    "async def rollout(client: openai.AsyncOpenAI, prompt: str) -> art.Trajectory:\n",
    "    messages: art.Messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    ]\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=messages, model=model.name, max_tokens=100\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    content = choice.message.content\n",
    "    assert isinstance(content, str)\n",
    "    if content == \"yes\":\n",
    "        reward = 0.5\n",
    "    elif content == \"no\":\n",
    "        reward = 0.75\n",
    "    elif content == \"maybe\":\n",
    "        reward = 1.0\n",
    "    else:\n",
    "        reward = 0.0\n",
    "    return art.Trajectory(messages_and_choices=[*messages, choice], reward=reward)\n",
    "\n",
    "\n",
    "async with model.openai_client(\n",
    "    estimated_completion_tokens=16, verbosity=2\n",
    ") as openai_client:\n",
    "    for i in range(await model.get_iteration(), 1_000):\n",
    "        train_groups = await art.gather_trajectories(\n",
    "            (\n",
    "                (rollout(openai_client, prompt) for _ in range(32))\n",
    "                for prompt in [\n",
    "                    f\"{prefix} with {', '.join([f\"'{w}'\" if use_quotes else w for w in words]) if len(words) == 3 else f'{words[0]}' + (f' or {words[1]}' if len(words) > 1 else '')}\"\n",
    "                    for prefix in [\"respond\", \"just respond\"]\n",
    "                    for use_quotes in [True, False]\n",
    "                    for words in [\n",
    "                        [\"yes\", \"no\", \"maybe\"],\n",
    "                        [\"maybe\", \"yes\", \"no\"],\n",
    "                        [\"no\", \"yes\", \"maybe\"],\n",
    "                        [\"yes\", \"maybe\", \"no\"],\n",
    "                        [\"yes\", \"no\"],\n",
    "                        [\"maybe\", \"no\"],\n",
    "                        [\"no\", \"maybe\"],\n",
    "                        [\"no\", \"yes\"],\n",
    "                        [\"yes\", \"no\"],\n",
    "                    ]\n",
    "                ]\n",
    "            ),\n",
    "            pbar_desc=\"train\",\n",
    "            stream_chat_completions=8,\n",
    "        )\n",
    "        await model.tune(\n",
    "            train_groups,\n",
    "            config=art.TuneConfig(\n",
    "                lr=5e-5, sequence_length=8192, plot_tensors=False, verbosity=2\n",
    "            ),\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
