{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-26 03:22:31 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import peft\n",
    "import transformers\n",
    "from typing import cast\n",
    "import vllm\n",
    "\n",
    "class CausallLM(transformers.PreTrainedModel, transformers.GenerationMixin):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.18: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 61.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 8192. Num Sequences = 226.\n",
      "Unsloth: vLLM's KV Cache can use up to 42.5 GB. Also swap space = 6 GB.\n",
      "INFO 03-26 03:22:39 config.py:549] This model supports multiple tasks: {'classify', 'generate', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 03-26 03:22:39 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":232}, use_cached_outputs=False, \n",
      "INFO 03-26 03:22:40 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 03-26 03:22:40 registry.py:335] `mm_limits` has already been set for model=unsloth/llama-3.1-8b-instruct-unsloth-bnb-4bit, and will be overwritten by the new values.\n",
      "INFO 03-26 03:22:40 model_runner.py:1110] Starting to load model unsloth/llama-3.1-8b-instruct-unsloth-bnb-4bit...\n",
      "INFO 03-26 03:22:40 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W326 03:22:40.103779710 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 03:22:41 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81b03b3d1864f8a85022dfb318d4e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bf334cc7e6421c8dd63e2afdf535bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 03:22:43 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
      "INFO 03-26 03:22:43 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 03-26 03:22:45 worker.py:267] Memory profiling takes 1.44 seconds\n",
      "INFO 03-26 03:22:45 worker.py:267] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.62) = 48.68GiB\n",
      "INFO 03-26 03:22:45 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 1.13GiB; the rest of the memory reserved for KV Cache is 41.80GiB.\n",
      "INFO 03-26 03:22:45 executor_base.py:111] # cuda blocks: 21401, # CPU blocks: 3072\n",
      "INFO 03-26 03:22:45 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 41.80x\n",
      "INFO 03-26 03:22:48 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 03:23:08 model_runner.py:1562] Graph capturing finished in 20 secs, took 4.17 GiB\n",
      "INFO 03-26 03:23:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 24.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.18 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (2-31): 30 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_rank = 32\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "model, tokenizer = cast(\n",
    "    tuple[CausallLM, transformers.PreTrainedTokenizerBase],\n",
    "    unsloth.FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=8192,\n",
    "        load_in_4bit=True,  # False for LoRA 16bit\n",
    "        fast_inference=True,  # Enable vLLM fast inference\n",
    "        # vLLM args\n",
    "        disable_log_requests=True,\n",
    "        disable_log_stats=False,\n",
    "        enable_prefix_caching=True,\n",
    "        gpu_memory_utilization=0.62,  # Reduce if out of memory\n",
    "        max_lora_rank=lora_rank,\n",
    "        # max_num_seqs=1024,\n",
    "        # enforce_eager=True,\n",
    "        num_scheduler_steps=16,\n",
    "        use_async=True,\n",
    "    ),\n",
    ")\n",
    "vllm_engine = cast(vllm.AsyncLLMEngine, model.vllm_engine)\n",
    "peft_model = cast(\n",
    "    peft.peft_model.PeftModelForCausalLM,\n",
    "    unsloth.FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],  # Remove QKVO if out of memory\n",
    "        lora_alpha=lora_rank,\n",
    "        # Enable long context finetuning\n",
    "        use_gradient_checkpointing=\"unsloth\",  # type: ignore\n",
    "        random_state=3407,\n",
    "    ),\n",
    ")\n",
    "lora_model = cast(peft.tuners.lora.LoraModel, peft_model.base_model)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_engine.engine.scheduler_config.max_num_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 0.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 1.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 2.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 3.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 4.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 5.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 6.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 7.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 8.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 9.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 10.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 11.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 12.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 13.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 14.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 15.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 16.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 17.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 18.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 19.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 20.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 21.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 22.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 23.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 24.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 25.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 26.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 27.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 28.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 29.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 30.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 31.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 32.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 33.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 34.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 35.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 36.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 37.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 38.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 39.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 40.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 41.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 42.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 43.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 44.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 45.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 46.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 47.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 48.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 49.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 50.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 51.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 52.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 53.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 54.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 55.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 56.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 57.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 58.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 59.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 60.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 61.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 62.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 63.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 64.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 65.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 66.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 67.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 68.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 69.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 70.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 71.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 72.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 73.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 74.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 75.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 76.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 77.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 78.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 79.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 80.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 81.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 82.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 83.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 84.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 85.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 86.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 87.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 88.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 89.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 90.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 91.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 92.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 93.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 94.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 95.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 96.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 97.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 98.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 99.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 100.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 101.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 102.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 103.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 104.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 105.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 106.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 107.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 108.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 109.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 110.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 111.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 112.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 113.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 114.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 115.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 116.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 117.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 118.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 119.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 120.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 121.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 122.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 123.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 124.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 125.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 126.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 127.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 128.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 129.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 130.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 131.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 132.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 133.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 134.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 135.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 136.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 137.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 138.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 139.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 140.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 141.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 142.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 143.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 144.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 145.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 146.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 147.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 148.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 149.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 150.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 151.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 152.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 153.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 154.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 155.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 156.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 157.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 158.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 159.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 160.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 161.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 162.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 163.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 164.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 165.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 166.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 167.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 168.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 169.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 170.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 171.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 172.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 173.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 174.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 175.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 176.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 177.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 178.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 179.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 180.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 181.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 182.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 183.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 184.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 185.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 186.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 187.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 188.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 189.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 190.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 191.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 192.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 193.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 194.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 195.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 196.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 197.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 198.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 199.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 200.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 201.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 202.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 203.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 204.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 205.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 206.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 207.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 208.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 209.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 210.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 211.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 212.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 213.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 214.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 215.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 216.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 217.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 218.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 219.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 220.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 221.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 222.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 223.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 224.\n",
      "INFO 03-26 03:23:15 async_llm_engine.py:211] Added request 225.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(vllm_engine.engine.scheduler_config.max_num_seqs):\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m vllm_engine.add_request(\n\u001b[32m     13\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, prompt=prompt, params=vllm.SamplingParams(max_tokens=\u001b[32m2\u001b[39m)\n\u001b[32m     14\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m vllm_engine.engine_step(\u001b[32m0\u001b[39m):\n\u001b[32m     17\u001b[39m     ...\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py:736\u001b[39m, in \u001b[36mAsyncLLMEngine.engine_step\u001b[39m\u001b[34m(self, virtual_engine)\u001b[39m\n\u001b[32m    733\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m new_request \u001b[38;5;129;01min\u001b[39;00m new_requests:\n\u001b[32m    734\u001b[39m     \u001b[38;5;66;03m# Add the request into the vLLM engine's waiting queue.\u001b[39;00m\n\u001b[32m    735\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine.add_request_async(**new_request)\n\u001b[32m    737\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    738\u001b[39m         \u001b[38;5;66;03m# TODO: use a vLLM specific error for failed validation\u001b[39;00m\n\u001b[32m    739\u001b[39m         \u001b[38;5;28mself\u001b[39m._request_tracker.process_exception(\n\u001b[32m    740\u001b[39m             new_request[\u001b[33m\"\u001b[39m\u001b[33mrequest_id\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    741\u001b[39m             e,\n\u001b[32m    742\u001b[39m             verbose=\u001b[38;5;28mself\u001b[39m.log_requests,\n\u001b[32m    743\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py:493\u001b[39m, in \u001b[36m_AsyncLLMEngine.add_request_async\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, trace_headers, prompt_adapter_request, priority, inputs)\u001b[39m\n\u001b[32m    490\u001b[39m     tokenizer = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tokenizer_async(lora_request)\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_token_prompt(prompt, tokenizer=tokenizer)\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m preprocessed_inputs = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_preprocessor.preprocess_async(\n\u001b[32m    494\u001b[39m     prompt,\n\u001b[32m    495\u001b[39m     request_id=request_id,\n\u001b[32m    496\u001b[39m     lora_request=lora_request,\n\u001b[32m    497\u001b[39m     prompt_adapter_request=prompt_adapter_request,\n\u001b[32m    498\u001b[39m )\n\u001b[32m    499\u001b[39m processed_inputs = \u001b[38;5;28mself\u001b[39m.input_processor(preprocessed_inputs)\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    502\u001b[39m     params.guided_decoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    503\u001b[39m     \u001b[38;5;66;03m# Guided decoding has an async implementation for building logits\u001b[39;00m\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# processors in a separate threadpool.\u001b[39;00m\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m# We want to invoke that here instead of using the blocking\u001b[39;00m\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# implementation in the LLMEngine\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/inputs/preprocess.py:790\u001b[39m, in \u001b[36mInputPreprocessor.preprocess_async\u001b[39m\u001b[34m(self, prompt, request_id, lora_request, prompt_adapter_request)\u001b[39m\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot pass encoder-decoder prompt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mto decoder-only models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Decoder-only operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_decoder_only_prompt_async(\n\u001b[32m    791\u001b[39m     prompt,\n\u001b[32m    792\u001b[39m     request_id=request_id,\n\u001b[32m    793\u001b[39m     lora_request=lora_request,\n\u001b[32m    794\u001b[39m     prompt_adapter_request=prompt_adapter_request,\n\u001b[32m    795\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/inputs/preprocess.py:730\u001b[39m, in \u001b[36mInputPreprocessor._process_decoder_only_prompt_async\u001b[39m\u001b[34m(self, prompt, request_id, lora_request, prompt_adapter_request)\u001b[39m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_process_decoder_only_prompt_async\u001b[39m(\n\u001b[32m    723\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    724\u001b[39m     prompt: SingletonPrompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    727\u001b[39m     prompt_adapter_request: Optional[PromptAdapterRequest] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    728\u001b[39m ) -> DecoderOnlyInputs:\n\u001b[32m    729\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m730\u001b[39m     prompt_comps = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._prompt_to_llm_inputs_async(\n\u001b[32m    731\u001b[39m         prompt,\n\u001b[32m    732\u001b[39m         request_id=request_id,\n\u001b[32m    733\u001b[39m         lora_request=lora_request,\n\u001b[32m    734\u001b[39m     )\n\u001b[32m    736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._build_decoder_only_llm_inputs(\n\u001b[32m    737\u001b[39m         prompt_comps,\n\u001b[32m    738\u001b[39m         prompt_adapter_request=prompt_adapter_request,\n\u001b[32m    739\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/inputs/preprocess.py:445\u001b[39m, in \u001b[36mInputPreprocessor._prompt_to_llm_inputs_async\u001b[39m\u001b[34m(self, prompt, request_id, lora_request)\u001b[39m\n\u001b[32m    437\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_process_multimodal():\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_multimodal_async(\n\u001b[32m    439\u001b[39m             prompt_text,\n\u001b[32m    440\u001b[39m             multi_modal_data,\n\u001b[32m    441\u001b[39m             mm_processor_kwargs,\n\u001b[32m    442\u001b[39m             lora_request=lora_request,\n\u001b[32m    443\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     prompt_token_ids = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenize_prompt_async(\n\u001b[32m    446\u001b[39m         prompt_text,\n\u001b[32m    447\u001b[39m         request_id=request_id,\n\u001b[32m    448\u001b[39m         lora_request=lora_request,\n\u001b[32m    449\u001b[39m     )\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token_inputs(\n\u001b[32m    452\u001b[39m         prompt=prompt_text,\n\u001b[32m    453\u001b[39m         prompt_token_ids=prompt_token_ids,\n\u001b[32m    454\u001b[39m         multi_modal_data=multi_modal_data,\n\u001b[32m    455\u001b[39m         mm_processor_kwargs=mm_processor_kwargs,\n\u001b[32m    456\u001b[39m     )\n\u001b[32m    458\u001b[39m assert_never(parsed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/inputs/preprocess.py:223\u001b[39m, in \u001b[36mInputPreprocessor._tokenize_prompt_async\u001b[39m\u001b[34m(self, prompt, request_id, lora_request)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.hf_config.model_type == \u001b[33m\"\u001b[39m\u001b[33mwhisper\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# For Whisper, special tokens should be provided by the user based\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# on the task and language of their request. Also needed to avoid\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# appending an EOS token to the prompt which disrupts generation.\u001b[39;00m\n\u001b[32m    222\u001b[39m     add_special_tokens = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m tokenizer.encode_async(\n\u001b[32m    224\u001b[39m     request_id=request_id,\n\u001b[32m    225\u001b[39m     prompt=prompt,\n\u001b[32m    226\u001b[39m     lora_request=lora_request,\n\u001b[32m    227\u001b[39m     add_special_tokens=add_special_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:76\u001b[39m, in \u001b[36mTokenizerGroup.encode_async\u001b[39m\u001b[34m(self, prompt, request_id, lora_request, add_special_tokens)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_async\u001b[39m(\n\u001b[32m     70\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     71\u001b[39m         prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     72\u001b[39m         request_id: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     73\u001b[39m         lora_request: Optional[LoRARequest] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     74\u001b[39m         add_special_tokens: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     75\u001b[39m     tokenizer = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_lora_tokenizer_async(lora_request)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     ret = \u001b[43mencode_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m                        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_if_input_too_long(ret, lora_request)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:58\u001b[39m, in \u001b[36mencode_tokens\u001b[39m\u001b[34m(tokenizer, text, add_special_tokens)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m add_special_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.encode(text, add_special_tokens=add_special_tokens)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2644\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, padding_side, return_tensors, **kwargs)\u001b[39m\n\u001b[32m   2606\u001b[39m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[32m   2607\u001b[39m     ENCODE_KWARGS_DOCSTRING,\n\u001b[32m   2608\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2627\u001b[39m     **kwargs,\n\u001b[32m   2628\u001b[39m ) -> List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   2629\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2630\u001b[39m \u001b[33;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[32m   2631\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2642\u001b[39m \u001b[33;03m            method).\u001b[39;00m\n\u001b[32m   2643\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2644\u001b[39m     encoded_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3063\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3053\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3054\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3055\u001b[39m     padding=padding,\n\u001b[32m   3056\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3060\u001b[39m     **kwargs,\n\u001b[32m   3061\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3063\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3066\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:613\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    590\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    591\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    610\u001b[39m     **kwargs,\n\u001b[32m    611\u001b[39m ) -> BatchEncoding:\n\u001b[32m    612\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    635\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    636\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:539\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    551\u001b[39m tokens_and_encodings = [\n\u001b[32m    552\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    553\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    562\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    563\u001b[39m ]\n",
      "\u001b[31mTypeError\u001b[39m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import torch\n",
    "\n",
    "prompt = {\n",
    "    \"prompt\": torch.tensor(\n",
    "        [0] * (vllm_engine.engine.scheduler_config.max_model_len - 2)\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "for i in range(vllm_engine.engine.scheduler_config.max_num_seqs):\n",
    "    await vllm_engine.add_request(\n",
    "        f\"{i}\", prompt=prompt, params=vllm.SamplingParams(max_tokens=2)\n",
    "    )\n",
    "\n",
    "while await vllm_engine.engine_step(0):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "54341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "peft.tuners.lora.model.LoraModel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(peft_model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import models\n",
    "models.llama.LlamaForCausalLM\n",
    "\n",
    "from typing import Protocol\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
