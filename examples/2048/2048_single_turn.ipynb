{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPipe client initialized\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from openpipe.client import OpenPipe\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "op_client = OpenPipe()\n",
    "print(\"OpenPipe client initialized\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "api = art.UnslothAPI(wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api.get_or_create_model(\n",
    "    name=\"2048-single-turn-003\", base_model=\"Qwen/Qwen2.5-7B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sky_workdir/src/art/__init__.py:28: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-09 18:38:41 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 46.17%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.1 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 320.\n",
      "Unsloth: vLLM's KV Cache can use up to 30.65 GB. Also swap space = 6 GB.\n",
      "INFO 04-09 18:38:54 config.py:549] This model supports multiple tasks: {'reward', 'score', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-09 18:38:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":320}, use_cached_outputs=False, \n",
      "INFO 04-09 18:38:55 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 04-09 18:38:55 registry.py:335] `mm_limits` has already been set for model=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, and will be overwritten by the new values.\n",
      "INFO 04-09 18:38:56 model_runner.py:1110] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 04-09 18:38:56 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 04-09 18:38:56 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 18:39:00 model_runner.py:1115] Loading model weights took 6.6961 GB\n",
      "INFO 04-09 18:39:00 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-09 18:39:04 worker.py:267] Memory profiling takes 4.17 seconds\n",
      "INFO 04-09 18:39:04 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.46) = 36.52GiB\n",
      "INFO 04-09 18:39:04 worker.py:267] model weights take 6.70GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 4.72GiB; the rest of the memory reserved for KV Cache is 24.96GiB.\n",
      "INFO 04-09 18:39:04 executor_base.py:111] # cuda blocks: 29213, # CPU blocks: 7021\n",
      "INFO 04-09 18:39:04 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 14.26x\n",
      "INFO 04-09 18:39:07 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:30<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 18:39:38 model_runner.py:1562] Graph capturing finished in 31 secs, took 1.14 GiB\n",
      "INFO 04-09 18:39:38 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 38.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9737f7edcd054c4aad61ca31b4497288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marctic_fly\u001b[0m (\u001b[33mbased-op\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/sky_workdir/examples/2048/wandb/run-20250409_184151-2048-single-turn-002</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/based-op/agent-reinforcement-training/runs/2048-single-turn-002' target=\"_blank\">2048-single-turn-002</a></strong> to <a href='https://wandb.ai/based-op/agent-reinforcement-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/based-op/agent-reinforcement-training' target=\"_blank\">https://wandb.ai/based-op/agent-reinforcement-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/based-op/agent-reinforcement-training/runs/2048-single-turn-002' target=\"_blank\">https://wandb.ai/based-op/agent-reinforcement-training/runs/2048-single-turn-002</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed 654 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c3af86becf4d8c9245c727fa0d19e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100,000 | Num Epochs = 3 | Total steps = 300,000\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 20,185,088/7,000,000,000 (0.29% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8516e5fe3e7e4da58e76efcae6013906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0000\n",
      "Packed 710 trajectories into 5 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80cecc4678b4ff9b21a16cc474be70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3eb67e374ca4cffa0f33ff9203e75d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0001\n",
      "Packed 654 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d41b66ec0d64abeb7ae430820740e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e29739915204956ad8592e8399d868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0002\n",
      "Packed 580 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3766fdd095499a84aee4e86fb35695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93e287cf90d4aa8a2f6cb5c358fedae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0003\n",
      "Packed 563 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72275088afa46a199f11080ad1c7097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5808ceec89a440ae96dc56436bc2f015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0004\n",
      "Packed 478 trajectories into 3 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0e5d4f01dc4e0f9fd8f7b9c2ccfa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a9c9a3c8a346a181a7844bb7b570b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0005\n",
      "Packed 554 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da70930fad314b87a9d4958f72d41de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c951da56294c4ab1541766dd6102fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0006\n",
      "Packed 440 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117aeaf45b3d444ca8f34952c8c888b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8f9dad111742f8a11ba7b15b7dd7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0007\n",
      "Packed 555 trajectories into 4 sequences of length 8192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632518d905fd4b14ba715867467a2934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tune:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5bf74d3f3e44bfbd6b77987a75ec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted iteration directory ./.art/models/2048-single-turn-002/0008\n",
      "Skipping tuning as there is no suitable data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e602bce7425c42839c45054f323bee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 282\u001b[39m\n\u001b[32m    279\u001b[39m openai_client = \u001b[38;5;28;01mawait\u001b[39;00m model.openai_client()\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m model.get_iteration(), \u001b[32m500\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     train_groups = \u001b[38;5;28;01mawait\u001b[39;00m art.gather_trajectories(\n\u001b[32m    283\u001b[39m         (\n\u001b[32m    284\u001b[39m             rollout(openai_client, i) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m18\u001b[39m)\n\u001b[32m    285\u001b[39m         ),\n\u001b[32m    286\u001b[39m         pbar_desc=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    287\u001b[39m         return_exceptions=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    288\u001b[39m     )\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# combine train_groups into a single list\u001b[39;00m\n\u001b[32m    291\u001b[39m     train_groups = [[item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m train_groups \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_trajectories.py:85\u001b[39m, in \u001b[36mgather_trajectories\u001b[39m\u001b[34m(groups, pbar_desc, pbar_total_completion_tokens, return_exceptions, stream_chat_completions, streaming_chat_completions_dir, clear_streaming_chat_completions_dir)\u001b[39m\n\u001b[32m     78\u001b[39m context = GroupsContext(\n\u001b[32m     79\u001b[39m     pbar=tqdm.tqdm(desc=pbar_desc, total=total),\n\u001b[32m     80\u001b[39m     pbar_total_completion_tokens=pbar_total_completion_tokens,\n\u001b[32m     81\u001b[39m     should_stream=\u001b[38;5;28miter\u001b[39m(cycle(should_stream)),\n\u001b[32m     82\u001b[39m     streaming_chat_completions_dir=streaming_chat_completions_dir,\n\u001b[32m     83\u001b[39m )\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_groups_context(context):\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     result_groups = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m     86\u001b[39m         *[\n\u001b[32m     87\u001b[39m             asyncio.gather(\n\u001b[32m     88\u001b[39m                 *[wrap_coroutine(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m g], return_exceptions=return_exceptions\n\u001b[32m     89\u001b[39m             )\n\u001b[32m     90\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups\n\u001b[32m     91\u001b[39m         ]\n\u001b[32m     92\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     94\u001b[39m     context.pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_trajectories.py:133\u001b[39m, in \u001b[36mwrap_coroutine\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    131\u001b[39m context.metric_sums[\u001b[33m\"\u001b[39m\u001b[33mexceptions\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m    132\u001b[39m context.update_pbar(n=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_trajectories.py:110\u001b[39m, in \u001b[36mwrap_coroutine\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    108\u001b[39m context = get_groups_context()\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m    111\u001b[39m     context.update_pbar(n=\u001b[32m1\u001b[39m)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Trajectory):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/utils/retry.py:81\u001b[39m, in \u001b[36mretry.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_attempts + \u001b[32m1\u001b[39m):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         last_exception = e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(client, iteration)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    195\u001b[39m         max_completion_tokens=\u001b[32m2048\u001b[39m,\n\u001b[32m    196\u001b[39m         messages=messages,\n\u001b[32m    197\u001b[39m         model=model.name,\n\u001b[32m    198\u001b[39m         temperature=\u001b[32m1.5\u001b[39m,\n\u001b[32m    199\u001b[39m     )\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m get_completion()\n\u001b[32m    203\u001b[39m     last_completion = chat_completion\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.LengthFinishReasonError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mrollout.<locals>.get_completion\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_completion\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    195\u001b[39m         max_completion_tokens=\u001b[32m2048\u001b[39m,\n\u001b[32m    196\u001b[39m         messages=messages,\n\u001b[32m    197\u001b[39m         model=model.name,\n\u001b[32m    198\u001b[39m         temperature=\u001b[32m1.5\u001b[39m,\n\u001b[32m    199\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:90\u001b[39m, in \u001b[36mpatch_openai.<locals>.create_patched\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m                 log_file.write(chunk.choices[\u001b[32m0\u001b[39m].delta.content \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     88\u001b[39m                 log_file.flush()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m consume_chat_completion_stream(\n\u001b[32m     91\u001b[39m             return_value, on_chunk\n\u001b[32m     92\u001b[39m         )\n\u001b[32m     93\u001b[39m report_usage(chat_completion)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:129\u001b[39m, in \u001b[36mconsume_chat_completion_stream\u001b[39m\u001b[34m(stream, on_chunk)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Consume a chat completion stream and build a complete ChatCompletion object.\u001b[39;00m\n\u001b[32m    112\u001b[39m \n\u001b[32m    113\u001b[39m \u001b[33;03mThis function processes a stream of ChatCompletionChunks, constructing a complete\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m \u001b[33;03m    AssertionError: If no chat completion object could be created.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    128\u001b[39m chat_completion: ChatCompletion | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chat_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    131\u001b[39m         chat_completion = ChatCompletion(\n\u001b[32m    132\u001b[39m             \u001b[38;5;28mid\u001b[39m=chunk.id,\n\u001b[32m    133\u001b[39m             choices=[\n\u001b[32m   (...)\u001b[39m\u001b[32m    144\u001b[39m             \u001b[38;5;28mobject\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mchat.completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    145\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:147\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:160\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m process_data = \u001b[38;5;28mself\u001b[39m._client._process_response_data\n\u001b[32m    158\u001b[39m iterator = \u001b[38;5;28mself\u001b[39m._iter_events()\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sse.data.startswith(\u001b[33m\"\u001b[39m\u001b[33m[DONE]\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:151\u001b[39m, in \u001b[36mAsyncStream._iter_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter_events\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[ServerSentEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.aiter_bytes(\u001b[38;5;28mself\u001b[39m.response.aiter_bytes()):\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m sse\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:302\u001b[39m, in \u001b[36mSSEDecoder.aiter_bytes\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maiter_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]) -> AsyncIterator[ServerSentEvent]:\n\u001b[32m    301\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiter_chunks(iterator):\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;00m\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m raw_line \u001b[38;5;129;01min\u001b[39;00m chunk.splitlines():\n\u001b[32m    305\u001b[39m             line = raw_line.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:313\u001b[39m, in \u001b[36mSSEDecoder._aiter_chunks\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[32m    312\u001b[39m data = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m chunk.splitlines(keepends=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    315\u001b[39m         data += line\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_models.py:997\u001b[39m, in \u001b[36mResponse.aiter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    995\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_raw():\n\u001b[32m    998\u001b[39m         decoded = decoder.decode(raw_bytes)\n\u001b[32m    999\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(decoded):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_models.py:1055\u001b[39m, in \u001b[36mResponse.aiter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1052\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_bytes_downloaded += \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_client.py:176\u001b[39m, in \u001b[36mBoundAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:271\u001b[39m, in \u001b[36mAsyncResponseStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    404\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._receive_response_body(**kwargs):\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:203\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py:289\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from art.utils.get_trajectory_messages import get_trajectory_messages\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "from typing import TypedDict\n",
    "import time\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "import math\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WINNING_VALUE = 512\n",
    "\n",
    "\n",
    "class TwentyFortyEightGame(TypedDict):\n",
    "    id: str\n",
    "    board: list[list[int | None]]\n",
    "\n",
    "def populate_random_cell(game: TwentyFortyEightGame) -> None:\n",
    "    all_clear_coordinates = [(i, j) for i in range(len(game[\"board\"])) for j in range(len(game[\"board\"][i])) if game[\"board\"][i][j] is None]\n",
    "    random_clear_coordinates = random.choice(all_clear_coordinates)\n",
    "    # 90% chance to populate a 2, 10% chance to populate a 4\n",
    "    game[\"board\"][random_clear_coordinates[0]][random_clear_coordinates[1]] = 2 if random.random() < 0.9 else 4\n",
    "\n",
    "\n",
    "def generate_game(board_length: int = 4) -> TwentyFortyEightGame:\n",
    "    # random 6 character string\n",
    "    id = ''.join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "    game = {\n",
    "        \"id\": id,\n",
    "        \"board\": [[None for _ in range(board_length)] for _ in range(board_length)]\n",
    "    }\n",
    "\n",
    "    # populate two random cells\n",
    "    populate_random_cell(game)\n",
    "    populate_random_cell(game)\n",
    "    \n",
    "    return game\n",
    "\n",
    "\n",
    "def render_board(game: TwentyFortyEightGame) -> str:\n",
    "    board = game[\"board\"]\n",
    "    # print something like this:\n",
    "    # _    | 2    | _    | 4\n",
    "    # 4    | 8    | 2    | 16\n",
    "    # 16   | 32   | 64   | 128\n",
    "    # _    | 2    | 2    | 4\n",
    "    # where _ is an empty cell\n",
    "\n",
    "    max_cell_width = max([len(str(cell)) for row in board for cell in row if cell is not None])\n",
    "\n",
    "    board_str = \"\"\n",
    "    for row in board:\n",
    "        # pad the cells with spaces to make them the same width\n",
    "        board_str += \"|\".join([str(cell).rjust(max_cell_width) if cell is not None else \"_\".rjust(max_cell_width) for cell in row])\n",
    "        board_str += \"\\n\"\n",
    "    return board_str\n",
    "\n",
    "# condense, privileging matches at the start of the sequence\n",
    "# sequences should be passed starting with cells that are the furthest in the direction in which the board is being condensed\n",
    "def condense_sequence(sequence: list[int | None]) -> list[int | None]:\n",
    "    condensed_sequence = []\n",
    "    \n",
    "    gapless_sequence = [cell for cell in sequence if cell is not None]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(gapless_sequence):\n",
    "        if i + 1 < len(gapless_sequence) and gapless_sequence[i] == gapless_sequence[i + 1]:\n",
    "            condensed_sequence.append(gapless_sequence[i] * 2)\n",
    "            i += 2\n",
    "        else:\n",
    "            condensed_sequence.append(gapless_sequence[i])\n",
    "            i += 1\n",
    "\n",
    "    # pad the sequence with None at the end\n",
    "    return condensed_sequence + [None] * (4 - len(condensed_sequence))\n",
    "\n",
    "def condense_board(game: TwentyFortyEightGame, direction: Literal[\"left\", \"right\", \"up\", \"down\"]) -> None:\n",
    "\n",
    "    if direction == \"left\":\n",
    "        for row in game[\"board\"]:\n",
    "            condensed_row = condense_sequence(row)\n",
    "            for i in range(len(row)):\n",
    "                row[i] = condensed_row[i]\n",
    "    \n",
    "    if direction == \"right\":\n",
    "        for row in game[\"board\"]:\n",
    "            reversed_row = row[::-1]\n",
    "            # reverse the row before and after condensing\n",
    "            condensed_row = condense_sequence(reversed_row)[::-1]\n",
    "            for i in range(len(row)):\n",
    "                row[i] = condensed_row[i]\n",
    "\n",
    "    if direction == \"up\":\n",
    "        for col_index in range(len(game[\"board\"][0])):\n",
    "            column = [row[col_index] for row in game[\"board\"]]\n",
    "\n",
    "            condensed_column = condense_sequence(column)\n",
    "            for row_index in range(len(column)):\n",
    "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
    "    \n",
    "    if direction == \"down\":\n",
    "        for col_index in range(len(game[\"board\"][0])):\n",
    "            column = [row[col_index] for row in game[\"board\"]]\n",
    "            reversed_column = column[::-1]\n",
    "            condensed_column = condense_sequence(reversed_column)[::-1]\n",
    "            for row_index in range(len(column)):\n",
    "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
    "        \n",
    "        \n",
    "\n",
    "class AgentMove(BaseModel):\n",
    "    direction: Literal[\"left\", \"right\", \"up\", \"down\"]\n",
    "\n",
    "\n",
    "def apply_agent_move(game: TwentyFortyEightGame, move_xml: str) -> None:\n",
    "\n",
    "    direction = None\n",
    "    # parse the move\n",
    "    try:\n",
    "        root = ET.fromstring(move_xml)\n",
    "        direction = root.text\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Invalid xml\")\n",
    "    \n",
    "    if direction not in [\"left\", \"right\", \"up\", \"down\"]:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "\n",
    "    condense_board(game, direction)\n",
    "\n",
    "    populate_random_cell(game)\n",
    "\n",
    "def max_cell_value(game: TwentyFortyEightGame) -> int:\n",
    "    return max([cell for row in game[\"board\"] for cell in row if cell is not None])\n",
    "\n",
    "\n",
    "def check_game_finished(game: TwentyFortyEightGame) -> bool:\n",
    "\n",
    "    if max_cell_value(game) >= WINNING_VALUE:\n",
    "        return True\n",
    "\n",
    "    # check if any cell is empty\n",
    "    if any(cell is None for row in game[\"board\"] for cell in row):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "failing_trajectory = None\n",
    "\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError, requests.ReadTimeout))\n",
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, iteration: int\n",
    ") -> art.Trajectory:\n",
    "\n",
    "    game = generate_game()\n",
    "\n",
    "    reward = 0\n",
    "    move_number = 0\n",
    "\n",
    "\n",
    "    trajectories: list[art.Trajectory] = []\n",
    "\n",
    "    directions = [\"left\", \"right\", \"up\", \"down\"]\n",
    "\n",
    "    while True:\n",
    "        # randomize directions to avoid bias\n",
    "        random.shuffle(directions)\n",
    "        directions_str = \"', '\".join(directions)\n",
    "\n",
    "        trajectory = art.Trajectory(\n",
    "            messages_and_choices=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are '{directions_str}'. Return your move as an XML object with a single property 'move', like so: <move>direction</move>\",\n",
    "                }\n",
    "            ],\n",
    "            reward=0,\n",
    "        )\n",
    "        trajectories.append(trajectory)\n",
    "                    \n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = get_trajectory_messages(trajectory)\n",
    "\n",
    "        async def get_completion():\n",
    "            return await client.chat.completions.create(\n",
    "                max_completion_tokens=2048,\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "                temperature=1.5,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "        \n",
    "        try:\n",
    "            op_client.report(\n",
    "                requested_at=requested_at,\n",
    "                received_at=int(time.time() * 1000),\n",
    "                req_payload={\n",
    "                    \"model\": model.name,\n",
    "                    \"messages\": messages,\n",
    "                    \"metadata\": {\n",
    "                        \"game_id\": game[\"id\"],\n",
    "                        \"notebook-id\": \"2048\",\n",
    "                        \"iteration\": str(iteration),\n",
    "                        \"move_number\": str(move_number),\n",
    "                    },\n",
    "                },\n",
    "                resp_payload=chat_completion,\n",
    "                status_code=200,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "            move_number += 1\n",
    "        except ValueError:\n",
    "            reward = -1\n",
    "            break\n",
    "\n",
    "        if check_game_finished(game):\n",
    "            max_value = max_cell_value(game)\n",
    "\n",
    "            if max_value < WINNING_VALUE:\n",
    "                # scale reward logarithmically between 0 for 2 and 1 for 2048\n",
    "                reward = (math.log(max_value, 2) - 1) / (math.log(WINNING_VALUE, 2) - 1)\n",
    "            else:\n",
    "                # double reward if it wins\n",
    "                reward = 2\n",
    "            break\n",
    "\n",
    "    for trajectory in trajectories:\n",
    "        trajectory.reward = reward\n",
    "    \n",
    "    try:\n",
    "        op_client.update_log_metadata(\n",
    "            filters=[\n",
    "                {\n",
    "                    \"field\": \"completionId\",\n",
    "                    \"equals\": last_completion.id,\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"reward\": str(reward),\n",
    "                \"reward_assigned\": \"true\",\n",
    "            }\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "persistent_client = None\n",
    "\n",
    "openai_client = await model.openai_client()\n",
    "\n",
    "for i in range(await model.get_iteration(), 500):\n",
    "    train_groups = await art.gather_trajectories(\n",
    "        (\n",
    "            rollout(openai_client, i) for _ in range(18)\n",
    "        ),\n",
    "        pbar_desc=\"train\",\n",
    "        return_exceptions=False,\n",
    "    )\n",
    "\n",
    "    # combine train_groups into a single list\n",
    "    train_groups = [[item for sublist in train_groups for item in sublist]]\n",
    "\n",
    "    await model.clear_iterations()\n",
    "    await model.tune(\n",
    "        train_groups, config=art.TuneConfig(lr=3e-5)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
