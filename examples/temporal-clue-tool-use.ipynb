{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-3-Llama-3.1-8B --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-num-seqs=4096 --max-num-batched-tokens=16384 --num-scheduler-steps=16 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --enable-auto-tool-choice --tool-call-parser=hermes --served-model-name=temporal-clue-tool-use-001 --port=8000 --api-key=default\n",
      "INFO 03-12 21:30:12 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 21:30:12 api_server.py:912] vLLM API server version 0.7.3\n",
      "INFO 03-12 21:30:12 api_server.py:913] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-3-Llama-3.1-8B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, enable_reasoning=False, reasoning_parser=None, tool_call_parser='hermes', tool_parser_plugin='', model='NousResearch/Hermes-3-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=80.0, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=16384, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=16, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode='swap', served_model_name=['temporal-clue-tool-use-001'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7080975a3920>)\n",
      "INFO 03-12 21:30:12 api_server.py:209] Started engine process with PID 85312\n",
      "INFO 03-12 21:30:17 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 21:30:19 config.py:549] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-12 21:30:19 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 03-12 21:30:19 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-12 21:30:19 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-12 21:30:24 config.py:549] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 03-12 21:30:24 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 03-12 21:30:24 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-12 21:30:24 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-12 21:30:24 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='NousResearch/Hermes-3-Llama-3.1-8B', speculative_config=None, tokenizer='NousResearch/Hermes-3-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=temporal-clue-tool-use-001, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 03-12 21:30:26 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 03-12 21:30:26 registry.py:335] `mm_limits` has already been set for model=NousResearch/Hermes-3-Llama-3.1-8B, and will be overwritten by the new values.\n",
      "INFO 03-12 21:30:27 model_runner.py:1110] Starting to load model NousResearch/Hermes-3-Llama-3.1-8B...\n",
      "INFO 03-12 21:30:27 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.10it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.06it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 21:30:31 model_runner.py:1115] Loading model weights took 14.9888 GB\n",
      "INFO 03-12 21:30:33 worker.py:267] Memory profiling takes 1.82 seconds\n",
      "INFO 03-12 21:30:33 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.95) = 75.14GiB\n",
      "INFO 03-12 21:30:33 worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 18.75GiB; the rest of the memory reserved for KV Cache is 41.26GiB.\n",
      "INFO 03-12 21:30:33 executor_base.py:111] # cuda blocks: 10563, # CPU blocks: 20480\n",
      "INFO 03-12 21:30:33 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 2.58x\n",
      "INFO 03-12 21:31:24 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 52.92 seconds\n",
      "INFO 03-12 21:31:24 serving_chat.py:76] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\n",
      "INFO 03-12 21:31:24 api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000\n",
      "INFO 03-12 21:31:24 launcher.py:23] Available routes are:\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /openapi.json, Methods: HEAD, GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /docs, Methods: HEAD, GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /redoc, Methods: HEAD, GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /health, Methods: GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /ping, Methods: POST, GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /tokenize, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /detokenize, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/models, Methods: GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /version, Methods: GET\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/completions, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/embeddings, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /pooling, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /score, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/score, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /rerank, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v1/rerank, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /v2/rerank, Methods: POST\n",
      "INFO 03-12 21:31:24 launcher.py:31] Route: /invocations, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [85187]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 21:31:31 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO 03-12 21:31:32 metrics.py:455] Avg prompt throughput: 2.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "INFO 03-12 21:31:32 metrics.py:471] Prefix cache hit rate: GPU: 0.00%, CPU: 0.00%\n",
      "INFO:     127.0.0.1:50498 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d956bef580f5437286f462988a24491d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2061f311c1734b37a503ad06fc93fb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"clues\": [\"The suspect motivated by Betrayal was in the Kitchen at 12:00 AM\", \"The murderer moved from the Kitchen to the Cloak Room at 12:00 AM\", \"The Horseshoe was in the room just east of the Rope at 12:00 AM\", \"The Horseshoe was in the room just east of Mr. Boddy at 11:30 PM\", \"Miss Scarlet was in the room just east of Mr. Boddy at 12:00 AM\", \"The Rope was in the room just west of Miss Scarlet at 11\n",
      "None\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m model.get_iteration(), \u001b[32m1_000\u001b[39m):\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m model.openai_client(tool_use=\u001b[38;5;28;01mTrue\u001b[39;00m, verbosity=\u001b[32m2\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m openai_client:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         val_groups, train_groups = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    153\u001b[39m             art.gather_groups(\n\u001b[32m    154\u001b[39m                 (\n\u001b[32m    155\u001b[39m                     (rollout(openai_client, puzzle) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m))\n\u001b[32m    156\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m puzzle \u001b[38;5;129;01min\u001b[39;00m val_puzzles\n\u001b[32m    157\u001b[39m                 ),\n\u001b[32m    158\u001b[39m                 pbar_desc=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    159\u001b[39m             ),\n\u001b[32m    160\u001b[39m             art.gather_groups(\n\u001b[32m    161\u001b[39m                 (\n\u001b[32m    162\u001b[39m                     (rollout(openai_client, puzzle) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m))\n\u001b[32m    163\u001b[39m                     \u001b[38;5;28;01mfor\u001b[39;00m puzzle \u001b[38;5;129;01min\u001b[39;00m train_puzzles[i * stride : (i + \u001b[32m1\u001b[39m) * stride]\n\u001b[32m    164\u001b[39m                 ),\n\u001b[32m    165\u001b[39m                 pbar_desc=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    166\u001b[39m             ),\n\u001b[32m    167\u001b[39m         )\n\u001b[32m    168\u001b[39m     _, _ = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    169\u001b[39m         model.save(val_groups),\n\u001b[32m    170\u001b[39m         model.tune(train_groups, config=art.TuneConfig(plot_tensors=\u001b[38;5;28;01mTrue\u001b[39;00m, verbosity=\u001b[32m2\u001b[39m)),\n\u001b[32m    171\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_groups.py:27\u001b[39m, in \u001b[36mgather_groups\u001b[39m\u001b[34m(groups, pbar_desc, pbar_total_completion_tokens)\u001b[39m\n\u001b[32m     22\u001b[39m context = GroupsContext(\n\u001b[32m     23\u001b[39m     pbar=tqdm.tqdm(desc=pbar_desc, total=\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups)),\n\u001b[32m     24\u001b[39m     pbar_total_completion_tokens=pbar_total_completion_tokens,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_groups_context(context):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     result_groups = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m     28\u001b[39m         *[asyncio.gather(*[wrap_coroutine(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m g]) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups]\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     31\u001b[39m     context.pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_groups.py:36\u001b[39m, in \u001b[36mwrap_coroutine\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_coroutine\u001b[39m(coro: Coroutine[Any, Any, T]) -> T:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     37\u001b[39m     context = get_groups_context()\n\u001b[32m     38\u001b[39m     context.update_pbar(n=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(client, puzzle)\u001b[39m\n\u001b[32m     37\u001b[39m messages: art.Messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: puzzle[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]}]\n\u001b[32m     38\u001b[39m tools: art.Tools = [\n\u001b[32m     39\u001b[39m     {\n\u001b[32m     40\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m     }\n\u001b[32m     57\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m     59\u001b[39m     messages=messages,\n\u001b[32m     60\u001b[39m     model=model.name,\n\u001b[32m     61\u001b[39m     tools=tools,\n\u001b[32m     62\u001b[39m )\n\u001b[32m     63\u001b[39m choice = chat_completion.choices[\u001b[32m0\u001b[39m]\n\u001b[32m     64\u001b[39m messages_and_choices = [*messages, choice]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:1041\u001b[39m, in \u001b[36mpatch_openai.<locals>.create_patched\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m         context.metric_sums[\u001b[33m\"\u001b[39m\u001b[33mtotal_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28msum\u001b[39m(\n\u001b[32m   1034\u001b[39m             \u001b[38;5;28mlen\u001b[39m(choice.logprobs.content \u001b[38;5;129;01mor\u001b[39;00m choice.logprobs.refusal \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chunk.choices\n\u001b[32m   1036\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m choice.logprobs\n\u001b[32m   1037\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (choice.logprobs.content \u001b[38;5;129;01mor\u001b[39;00m choice.logprobs.refusal)\n\u001b[32m   1038\u001b[39m         )\n\u001b[32m   1039\u001b[39m         context.update_pbar(n=\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m consume_chat_completion_stream(return_value, on_chunk)\n\u001b[32m   1042\u001b[39m report_usage(chat_completion)\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:1071\u001b[39m, in \u001b[36mconsume_chat_completion_stream\u001b[39m\u001b[34m(stream, on_chunk)\u001b[39m\n\u001b[32m   1053\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Consume a chat completion stream and build a complete ChatCompletion object.\u001b[39;00m\n\u001b[32m   1054\u001b[39m \n\u001b[32m   1055\u001b[39m \u001b[33;03mThis function processes a stream of ChatCompletionChunks, constructing a complete\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1068\u001b[39m \u001b[33;03m    AssertionError: If no chat completion object could be created.\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1070\u001b[39m chat_completion: ChatCompletion | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chat_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1073\u001b[39m         chat_completion = ChatCompletion(\n\u001b[32m   1074\u001b[39m             \u001b[38;5;28mid\u001b[39m=chunk.id,\n\u001b[32m   1075\u001b[39m             choices=[\n\u001b[32m   (...)\u001b[39m\u001b[32m   1086\u001b[39m             \u001b[38;5;28mobject\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mchat.completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1087\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:147\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:160\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m process_data = \u001b[38;5;28mself\u001b[39m._client._process_response_data\n\u001b[32m    158\u001b[39m iterator = \u001b[38;5;28mself\u001b[39m._iter_events()\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sse.data.startswith(\u001b[33m\"\u001b[39m\u001b[33m[DONE]\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:151\u001b[39m, in \u001b[36mAsyncStream._iter_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter_events\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[ServerSentEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.aiter_bytes(\u001b[38;5;28mself\u001b[39m.response.aiter_bytes()):\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m sse\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:302\u001b[39m, in \u001b[36mSSEDecoder.aiter_bytes\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maiter_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]) -> AsyncIterator[ServerSentEvent]:\n\u001b[32m    301\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiter_chunks(iterator):\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;00m\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m raw_line \u001b[38;5;129;01min\u001b[39;00m chunk.splitlines():\n\u001b[32m    305\u001b[39m             line = raw_line.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:313\u001b[39m, in \u001b[36mSSEDecoder._aiter_chunks\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[32m    312\u001b[39m data = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m chunk.splitlines(keepends=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    315\u001b[39m         data += line\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_models.py:997\u001b[39m, in \u001b[36mResponse.aiter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    995\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_raw():\n\u001b[32m    998\u001b[39m         decoded = decoder.decode(raw_bytes)\n\u001b[32m    999\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(decoded):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_models.py:1055\u001b[39m, in \u001b[36mResponse.aiter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1052\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_bytes_downloaded += \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_client.py:176\u001b[39m, in \u001b[36mBoundAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:271\u001b[39m, in \u001b[36mAsyncResponseStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    404\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._receive_response_body(**kwargs):\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:203\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1246\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1241\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1242\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1243\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1244\u001b[39m ):\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "puzzles: list[TemporalCluePuzzle] = json.load(open(\"./data/temporal-clue/puzzles.json\"))\n",
    "val_puzzles = puzzles[:64]\n",
    "test_puzzles = puzzles[64:128]\n",
    "train_puzzles = puzzles[128:]\n",
    "random.seed(42)\n",
    "random.shuffle(train_puzzles)\n",
    "\n",
    "\n",
    "api = art.LocalAPI(wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api.get_or_create_model(\n",
    "    name=\"temporal-clue-tool-use-001\",\n",
    "    base_model=\"NousResearch/Hermes-3-Llama-3.1-8B\",\n",
    ")\n",
    "\n",
    "\n",
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, puzzle: TemporalCluePuzzle\n",
    ") -> art.Trajectory:\n",
    "    messages: art.Messages = [{\"role\": \"user\", \"content\": puzzle[\"prompt\"]}]\n",
    "    tools: art.Tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_hints\",\n",
    "                \"description\": \"A function to retrieve one or two hints. No more than 3 hints may be retrieved total. \"\n",
    "                \"Each retrieved hint decreases your final accuracy score by 10%.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"num_hints\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Number of hints to retrieve (1 or 2)\",\n",
    "                            \"enum\": [1, 2],\n",
    "                        }\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model.name,\n",
    "        tools=tools,\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    messages_and_choices = [*messages, choice]\n",
    "    hints = [\n",
    "        f\"The answer for {key} is {value}\" for key, value in puzzle[\"solution\"].items()\n",
    "    ]\n",
    "    random.shuffle(hints)\n",
    "    hints_shared = 0\n",
    "\n",
    "    def get_hints(function_name: str, function_arguments: str) -> str:\n",
    "        nonlocal hints_shared\n",
    "        if function_name != \"get_hints\":\n",
    "            return f\"Error: unexpected function name {function_name}\"\n",
    "        try:\n",
    "            num_hints = json.loads(function_arguments or \"{}\").get(\"num_hints\", 1)\n",
    "        except Exception:\n",
    "            return f\"Error: invalid JSON {function_arguments}\"\n",
    "        if num_hints not in {1, 2}:\n",
    "            return f\"Error: invalid number of hints {num_hints}\"\n",
    "        if num_hints + hints_shared > 3:\n",
    "            return f\"Error: cannot retrieve {num_hints} hints, already retrieved {hints_shared} hints\"\n",
    "        hints_shared += num_hints\n",
    "        content = \"Hints:\"\n",
    "        for _ in range(num_hints):\n",
    "            content += f\"\\n{hints.pop()}\"\n",
    "        return content\n",
    "\n",
    "    while tool_calls := choice.message.tool_calls:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": choice.message.content,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tool_call.id,\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"arguments\": tool_call.function.arguments or \"{}\",\n",
    "                        },\n",
    "                    }\n",
    "                    for tool_call in tool_calls\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.function.arguments:\n",
    "                print(tool_call.function.arguments)\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": get_hints(\n",
    "                        tool_call.function.name, tool_call.function.arguments\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "            messages_and_choices.append(messages[-1])\n",
    "        try:\n",
    "            chat_completion = await client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "                tools=tools,\n",
    "            )\n",
    "        except openai.BadRequestError:\n",
    "            # Likely incorrectly formatted tool call arguments. We'll break\n",
    "            # out of the loop and allow the model to (probably) fail.\n",
    "            print(messages[-1].get(\"tool_calls\"))\n",
    "            break\n",
    "        choice = chat_completion.choices[0]\n",
    "        messages_and_choices.append(choice)\n",
    "\n",
    "    content = choice.message.content or \"\"\n",
    "    num_correct = 0\n",
    "    for key, value in puzzle[\"solution\"].items():\n",
    "        if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "            match = matches[-1]\n",
    "            if match.strip().lower() == value.lower():\n",
    "                num_correct += 1\n",
    "    reward = acc = num_correct / len(puzzle[\"solution\"])\n",
    "    return art.Trajectory(\n",
    "        messages_and_choices=messages_and_choices,\n",
    "        reward=reward - hints_shared * 0.1,\n",
    "        metrics={\"acc\": acc, \"hints_shared\": hints_shared},\n",
    "    )\n",
    "\n",
    "\n",
    "stride = 32\n",
    "for i in range(await model.get_iteration(), 1_000):\n",
    "    async with model.openai_client(tool_use=True, verbosity=2) as openai_client:\n",
    "        val_groups, train_groups = await asyncio.gather(\n",
    "            art.gather_groups(\n",
    "                (\n",
    "                    (rollout(openai_client, puzzle) for _ in range(2))\n",
    "                    for puzzle in val_puzzles\n",
    "                ),\n",
    "                pbar_desc=\"val\",\n",
    "            ),\n",
    "            art.gather_groups(\n",
    "                (\n",
    "                    (rollout(openai_client, puzzle) for _ in range(50))\n",
    "                    for puzzle in train_puzzles[i * stride : (i + 1) * stride]\n",
    "                ),\n",
    "                pbar_desc=\"train\",\n",
    "            ),\n",
    "        )\n",
    "    _, _ = await asyncio.gather(\n",
    "        model.save(val_groups),\n",
    "        model.tune(train_groups, config=art.TuneConfig(plot_tensors=True, verbosity=2)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{bos_token}}{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "tools: art.Tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_hints\",\n",
    "            \"description\": \"A function to retrieve one or two hints. No more than 3 hints may be retrieved total. \"\n",
    "            \"Each retrieved hint decreases your final accuracy score by 10%.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"num_hints\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Number of hints to retrieve (1 or 2)\",\n",
    "                        \"enum\": [1, 2],\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\n",
    "    tokenizer.get_chat_template()\n",
    "    # .replace(\n",
    "    #     ' or (message.role == \"assistant\" and message.tool_calls is not defined)', \"\"\n",
    "    # )\n",
    "    # .replace(\n",
    "    #     '{%- elif message.role == \"assistant\" %}',\n",
    "    #     \"{%- elif message.role == \\\"assistant\\\" and message.tool_calls is not defined %}\\n        {{- '<|im_start|>' + message.role + '\\n' }}{% generate %}{{ message.content + '<|im_end|>' }}{% endgenerate %}{{ '\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\",\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|im_start|>system\n",
      "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"get_hints\", \"description\": \"get_hints(num_hints: int) - A function to retrieve one or two hints. No more than 3 hints may be retrieved total. Each retrieved hint decreases your final accuracy score by 10%.\n",
      "\n",
      "    Args:\n",
      "        num_hints(int): Number of hints to retrieve (1 or 2)\", \"parameters\": {\"type\": \"object\", \"properties\": {\"num_hints\": {\"type\": \"integer\", \"description\": \"Number of hints to retrieve (1 or 2)\", \"enum\": [1, 2]}}}} </tools>Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}}, \"required\": [\"name\", \"arguments\"], \"title\": \"FunctionCall\", \"type\": \"object\"}}\n",
      "For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "<tool_call>\n",
      "{\"name\": <function-name>, \"arguments\": <args-dict>}\n",
      "</tool_call><|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hello, world!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<tool_call>\n",
      "{\"name\": \"get_hints\", \"arguments\": {}}\n",
      "</tool_call><|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"assistant\", \"content\": \"Hello, world!\"},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Hello, world!\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"1\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\":{\"name\": \"get_hints\", \"arguments\": \"{}\"},\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        tools=tools,\n",
    "        chat_template=tokenizer.get_chat_template(\"tool_use\"),\n",
    "        tokenize=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ vllm serve NousResearch/Hermes-3-Llama-3.1-8B --block-size=32 --disable-log-requests --enable-chunked-prefill --enable-prefix-caching --enforce-eager --gpu-memory-utilization=0.95 --max-num-seqs=4096 --max-num-batched-tokens=16384 --num-scheduler-steps=16 --preemption-mode=swap --return-tokens-as-token-ids --swap-space=80 --tensor-parallel-size=1 --enable-auto-tool-choice --tool-call-parser=hermes --chat-template=./tool_chat_template_hermes.jinja --served-model-name=temporal-clue-tool-use-001 --port=8001 --api-key=default\n",
      "INFO 03-12 20:47:42 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 20:47:42 api_server.py:912] vLLM API server version 0.7.3\n",
      "INFO 03-12 20:47:42 api_server.py:913] args: Namespace(subparser='serve', model_tag='NousResearch/Hermes-3-Llama-3.1-8B', config='', host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='default', lora_modules=None, prompt_adapters=None, chat_template='./tool_chat_template_hermes.jinja', chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=True, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=True, enable_reasoning=False, reasoning_parser=None, tool_call_parser='hermes', tool_parser_plugin='', model='NousResearch/Hermes-3-Llama-3.1-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=32, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=80.0, cpu_offload_gb=0, gpu_memory_utilization=0.95, num_gpu_blocks_override=None, max_num_batched_tokens=16384, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=4096, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=16, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode='swap', served_model_name=['temporal-clue-tool-use-001'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function ServeSubcommand.cmd at 0x7e8338397920>)\n",
      "INFO 03-12 20:47:42 api_server.py:209] Started engine process with PID 69227\n",
      "INFO 03-12 20:47:48 __init__.py:207] Automatically detected platform cuda.\n",
      "INFO 03-12 20:47:49 config.py:549] This model supports multiple tasks: {'score', 'reward', 'classify', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-12 20:47:49 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 03-12 20:47:49 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-12 20:47:49 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-12 20:47:54 config.py:549] This model supports multiple tasks: {'reward', 'score', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-12 20:47:54 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 03-12 20:47:54 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-12 20:47:54 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-12 20:47:54 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='NousResearch/Hermes-3-Llama-3.1-8B', speculative_config=None, tokenizer='NousResearch/Hermes-3-Llama-3.1-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=temporal-clue-tool-use-001, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True, \n",
      "INFO 03-12 20:47:56 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 03-12 20:47:56 registry.py:335] `mm_limits` has already been set for model=NousResearch/Hermes-3-Llama-3.1-8B, and will be overwritten by the new values.\n",
      "INFO 03-12 20:47:57 model_runner.py:1110] Starting to load model NousResearch/Hermes-3-Llama-3.1-8B...\n",
      "INFO 03-12 20:47:57 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.08it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.05it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 20:48:01 model_runner.py:1115] Loading model weights took 14.9888 GB\n",
      "INFO 03-12 20:48:03 worker.py:267] Memory profiling takes 1.68 seconds\n",
      "INFO 03-12 20:48:03 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.95) = 75.14GiB\n",
      "INFO 03-12 20:48:03 worker.py:267] model weights take 14.99GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 18.75GiB; the rest of the memory reserved for KV Cache is 41.26GiB.\n",
      "INFO 03-12 20:48:03 executor_base.py:111] # cuda blocks: 10563, # CPU blocks: 20480\n",
      "INFO 03-12 20:48:03 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 2.58x\n",
      "INFO 03-12 20:48:50 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 49.18 seconds\n",
      "INFO 03-12 20:48:51 api_server.py:820] Using supplied chat template:\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- macro json_to_python_type(json_spec) %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- set basic_type_map = {\n",
      "INFO 03-12 20:48:51 api_server.py:820]     \"string\": \"str\",\n",
      "INFO 03-12 20:48:51 api_server.py:820]     \"number\": \"float\",\n",
      "INFO 03-12 20:48:51 api_server.py:820]     \"integer\": \"int\",\n",
      "INFO 03-12 20:48:51 api_server.py:820]     \"boolean\": \"bool\"\n",
      "INFO 03-12 20:48:51 api_server.py:820] } %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] \n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- if basic_type_map[json_spec.type] is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- basic_type_map[json_spec.type] }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- elif json_spec.type == \"array\" %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \"list[\" +  json_to_python_type(json_spec|items) + \"]\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- elif json_spec.type == \"object\" %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if json_spec.additionalProperties is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \"dict[str, \" + json_to_python_type(json_spec.additionalProperties) + ']' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- else %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \"dict\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- elif json_spec.type is iterable %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \"Union[\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- for t in json_spec.type %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- json_to_python_type({\"type\": t}) }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- if not loop.last %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {{- \",\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endfor %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \"]\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- else %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \"Any\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- endmacro %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] \n",
      "INFO 03-12 20:48:51 api_server.py:820] \n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- bos_token }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- \"<|im_start|>system\\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- if tools is iterable and tools | length > 0 %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- for tool in tools %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if tool.function is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- set tool = tool.function %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '{\"type\": \"function\", \"function\": ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '{\"name\": \"' + tool.name + '\", ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '\"description\": \"' + tool.name + '(' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- for param_name, param_fields in tool.parameters.properties|items %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- param_name + \": \" + json_to_python_type(param_fields) }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- if not loop.last %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {{- \", \" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endfor %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \")\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if tool.return is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \" -> \" + json_to_python_type(tool.return) }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \" - \" + tool.description + \"\\n\\n\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- for param_name, param_fields in tool.parameters.properties|items %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- if loop.first %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {{- \"    Args:\\n\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \"        \" + param_name + \"(\" + json_to_python_type(param_fields) + \"): \" + param_fields.description|trim }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endfor %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if tool.return is defined and tool.return.description is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \"\\n    Returns:\\n        \" + tool.return.description }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '\"' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- ', \"parameters\": ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if tool.parameters.properties | length == 0 %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \"{}\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- else %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- tool.parameters|tojson }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- \"}\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if not loop.last %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- \"\\n\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- endfor %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- \" </tools>\" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- 'Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}}, \"required\": [\"name\", \"arguments\"], \"title\": \"FunctionCall\", \"type\": \"object\"}}\n",
      "INFO 03-12 20:48:51 api_server.py:820] ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- \"For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
      "INFO 03-12 20:48:51 api_server.py:820] \" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- \"<tool_call>\n",
      "INFO 03-12 20:48:51 api_server.py:820] \" }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- '{\"name\": <function-name>, \"arguments\": <args-dict>}\n",
      "INFO 03-12 20:48:51 api_server.py:820] ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {{- '</tool_call><|im_end|>' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- for message in messages %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- if message.role == \"user\" or message.role == \"system\" or (message.role == \"assistant\" and message.tool_calls is not defined) %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- elif message.role == \"assistant\" and message.tool_calls is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '<|im_start|>' + message.role }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- for tool_call in message.tool_calls %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '\\n<tool_call>\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- if tool_call.function is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {%- set tool_call = tool_call.function %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '{' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '\"name\": \"' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- tool_call.name }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '\"' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- if tool_call.arguments is defined %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {{- ', ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {{- '\"arguments\": ' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]                 {{- tool_call.arguments|tojson }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '}' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '\\n</tool_call>' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endfor %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '<|im_end|>\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- elif message.role == \"tool\" %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if loop.previtem and loop.previtem.role != \"tool\" %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '<|im_start|>tool\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- '<tool_response>\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {{- message.content }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if not loop.last %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '\\n</tool_response>\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- else %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '\\n</tool_response>' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- if not loop.last and loop.nextitem.role != \"tool\" %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '<|im_end|>' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- elif loop.last %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]             {{- '<|im_end|>' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820]         {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {%- endif %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- endfor %}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- if add_generation_prompt %}\n",
      "INFO 03-12 20:48:51 api_server.py:820]     {{- '<|im_start|>assistant\\n' }}\n",
      "INFO 03-12 20:48:51 api_server.py:820] {%- endif %}\n",
      "INFO 03-12 20:48:51 serving_chat.py:76] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\n",
      "INFO 03-12 20:48:51 api_server.py:958] Starting vLLM API server on http://0.0.0.0:8001\n",
      "INFO 03-12 20:48:51 launcher.py:23] Available routes are:\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /openapi.json, Methods: GET, HEAD\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /docs, Methods: GET, HEAD\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /redoc, Methods: GET, HEAD\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /health, Methods: GET\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /ping, Methods: GET, POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /tokenize, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /detokenize, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/models, Methods: GET\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /version, Methods: GET\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/chat/completions, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/completions, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/embeddings, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /pooling, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /score, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/score, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/audio/transcriptions, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /rerank, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v1/rerank, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /v2/rerank, Methods: POST\n",
      "INFO 03-12 20:48:51 launcher.py:31] Route: /invocations, Methods: POST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [69159]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-12 20:48:54 chat_utils.py:332] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
      "INFO:     127.0.0.1:54422 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "vLLM server started succesfully. Logs can be found at ./logs/vllm.log\n"
     ]
    }
   ],
   "source": [
    "context_manager = model.openai_client(verbosity=2)\n",
    "openai_client = await context_manager.gen.__anext__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aedca6940d74aec9604d7543e536567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac82f1affc294e458ebb945240193686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    107\u001b[39m     reward = acc = num_correct / \u001b[38;5;28mlen\u001b[39m(puzzle[\u001b[33m\"\u001b[39m\u001b[33msolution\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m art.Trajectory(\n\u001b[32m    109\u001b[39m         messages_and_choices=messages_and_choices,\n\u001b[32m    110\u001b[39m         reward=reward - hints_shared * \u001b[32m0.1\u001b[39m,\n\u001b[32m    111\u001b[39m         metrics={\u001b[33m\"\u001b[39m\u001b[33macc\u001b[39m\u001b[33m\"\u001b[39m: acc, \u001b[33m\"\u001b[39m\u001b[33mhints_shared\u001b[39m\u001b[33m\"\u001b[39m: hints_shared},\n\u001b[32m    112\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m val_groups, train_groups = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    116\u001b[39m     art.gather_groups(\n\u001b[32m    117\u001b[39m         ((rollout(openai_client, puzzle) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m puzzle \u001b[38;5;129;01min\u001b[39;00m val_puzzles),\n\u001b[32m    118\u001b[39m         pbar_desc=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     ),\n\u001b[32m    120\u001b[39m     art.gather_groups(\n\u001b[32m    121\u001b[39m         (\n\u001b[32m    122\u001b[39m             (rollout(openai_client, puzzle) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m))\n\u001b[32m    123\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m puzzle \u001b[38;5;129;01min\u001b[39;00m train_puzzles[i * stride : (i + \u001b[32m1\u001b[39m) * stride]\n\u001b[32m    124\u001b[39m         ),\n\u001b[32m    125\u001b[39m         pbar_desc=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    126\u001b[39m     ),\n\u001b[32m    127\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_groups.py:27\u001b[39m, in \u001b[36mgather_groups\u001b[39m\u001b[34m(groups, pbar_desc, pbar_total_completion_tokens)\u001b[39m\n\u001b[32m     22\u001b[39m context = GroupsContext(\n\u001b[32m     23\u001b[39m     pbar=tqdm.tqdm(desc=pbar_desc, total=\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(g) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups)),\n\u001b[32m     24\u001b[39m     pbar_total_completion_tokens=pbar_total_completion_tokens,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_groups_context(context):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     result_groups = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m     28\u001b[39m         *[asyncio.gather(*[wrap_coroutine(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m g]) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups]\n\u001b[32m     29\u001b[39m     )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     31\u001b[39m     context.pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather_groups.py:36\u001b[39m, in \u001b[36mwrap_coroutine\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_coroutine\u001b[39m(coro: Coroutine[Any, Any, T]) -> T:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[32m     37\u001b[39m     context = get_groups_context()\n\u001b[32m     38\u001b[39m     context.update_pbar(n=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(client, puzzle)\u001b[39m\n\u001b[32m      4\u001b[39m messages: art.Messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: puzzle[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m]}]\n\u001b[32m      5\u001b[39m tools: art.Tools = [\n\u001b[32m      6\u001b[39m     {\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     }\n\u001b[32m     24\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m     26\u001b[39m     messages=messages,\n\u001b[32m     27\u001b[39m     model=model.name,\n\u001b[32m     28\u001b[39m     tools=tools,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m choice = chat_completion.choices[\u001b[32m0\u001b[39m]\n\u001b[32m     31\u001b[39m messages_and_choices = [*messages, choice]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:1041\u001b[39m, in \u001b[36mpatch_openai.<locals>.create_patched\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m         context.metric_sums[\u001b[33m\"\u001b[39m\u001b[33mtotal_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28msum\u001b[39m(\n\u001b[32m   1034\u001b[39m             \u001b[38;5;28mlen\u001b[39m(choice.logprobs.content \u001b[38;5;129;01mor\u001b[39;00m choice.logprobs.refusal \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m   1035\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chunk.choices\n\u001b[32m   1036\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m choice.logprobs\n\u001b[32m   1037\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (choice.logprobs.content \u001b[38;5;129;01mor\u001b[39;00m choice.logprobs.refusal)\n\u001b[32m   1038\u001b[39m         )\n\u001b[32m   1039\u001b[39m         context.update_pbar(n=\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1041\u001b[39m chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m consume_chat_completion_stream(return_value, on_chunk)\n\u001b[32m   1042\u001b[39m report_usage(chat_completion)\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:20\u001b[39m, in \u001b[36mconsume_chat_completion_stream\u001b[39m\u001b[34m(stream, on_chunk)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:147\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:160\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    157\u001b[39m process_data = \u001b[38;5;28mself\u001b[39m._client._process_response_data\n\u001b[32m    158\u001b[39m iterator = \u001b[38;5;28mself\u001b[39m._iter_events()\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m    161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sse.data.startswith(\u001b[33m\"\u001b[39m\u001b[33m[DONE]\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    162\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:151\u001b[39m, in \u001b[36mAsyncStream._iter_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter_events\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[ServerSentEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m sse \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoder.aiter_bytes(\u001b[38;5;28mself\u001b[39m.response.aiter_bytes()):\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m sse\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:302\u001b[39m, in \u001b[36mSSEDecoder.aiter_bytes\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maiter_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterator: AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]) -> AsyncIterator[ServerSentEvent]:\n\u001b[32m    301\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiter_chunks(iterator):\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;00m\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m raw_line \u001b[38;5;129;01min\u001b[39;00m chunk.splitlines():\n\u001b[32m    305\u001b[39m             line = raw_line.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:313\u001b[39m, in \u001b[36mSSEDecoder._aiter_chunks\u001b[39m\u001b[34m(self, iterator)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[32m    312\u001b[39m data = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m chunk.splitlines(keepends=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    315\u001b[39m         data += line\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_models.py:997\u001b[39m, in \u001b[36mResponse.aiter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    995\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_raw():\n\u001b[32m    998\u001b[39m         decoded = decoder.decode(raw_bytes)\n\u001b[32m    999\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(decoded):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_models.py:1055\u001b[39m, in \u001b[36mResponse.aiter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1052\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_bytes_downloaded += \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_client.py:176\u001b[39m, in \u001b[36mBoundAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:271\u001b[39m, in \u001b[36mAsyncResponseStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    404\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._receive_response_body(**kwargs):\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:203\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1246\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1241\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1242\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1243\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1244\u001b[39m ):\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, puzzle: TemporalCluePuzzle\n",
    ") -> art.Trajectory:\n",
    "    messages: art.Messages = [{\"role\": \"user\", \"content\": puzzle[\"prompt\"]}]\n",
    "    tools: art.Tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_hints\",\n",
    "                \"description\": \"A function to retrieve one or two hints. No more than 3 hints may be retrieved total. \"\n",
    "                \"Each retrieved hint decreases your final accuracy score by 10%.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"num_hints\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Number of hints to retrieve (1 or 2)\",\n",
    "                            \"enum\": [1, 2],\n",
    "                        }\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model.name,\n",
    "        tools=tools,\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    messages_and_choices = [*messages, choice]\n",
    "    hints = [\n",
    "        f\"The answer for {key} is {value}\" for key, value in puzzle[\"solution\"].items()\n",
    "    ]\n",
    "    random.shuffle(hints)\n",
    "    hints_shared = 0\n",
    "\n",
    "    def get_hints(function_name: str, function_arguments: str) -> str:\n",
    "        nonlocal hints_shared\n",
    "        if function_name != \"get_hints\":\n",
    "            return f\"Error: unexpected function name {function_name}\"\n",
    "        try:\n",
    "            num_hints = json.loads(function_arguments or \"{}\").get(\"num_hints\", 1)\n",
    "        except Exception:\n",
    "            return f\"Error: invalid JSON {function_arguments}\"\n",
    "        if num_hints not in {1, 2}:\n",
    "            return f\"Error: invalid number of hints {num_hints}\"\n",
    "        if num_hints + hints_shared > 3:\n",
    "            return f\"Error: cannot retrieve {num_hints} hints, already retrieved {hints_shared} hints\"\n",
    "        hints_shared += num_hints\n",
    "        content = \"Hints:\"\n",
    "        for _ in range(num_hints):\n",
    "            content += f\"\\n{hints.pop()}\"\n",
    "        return content\n",
    "\n",
    "    while tool_calls := choice.message.tool_calls:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": choice.message.content,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tool_call.id,\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"arguments\": tool_call.function.arguments or \"{}\",\n",
    "                        },\n",
    "                    }\n",
    "                    for tool_call in tool_calls\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call.function.arguments:\n",
    "                print(tool_call.function.arguments)\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": get_hints(\n",
    "                        tool_call.function.name, tool_call.function.arguments\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "            messages_and_choices.append(messages[-1])\n",
    "        try:\n",
    "            chat_completion = await client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "                tools=tools,\n",
    "            )\n",
    "        except openai.BadRequestError:\n",
    "            # Likely incorrectly formatted tool call arguments. We'll break out of the loop and allow the model to fail.\n",
    "            print(messages[-1].get(\"tool_calls\"))\n",
    "            break\n",
    "        choice = chat_completion.choices[0]\n",
    "        messages_and_choices.append(choice)\n",
    "\n",
    "    content = choice.message.content or \"\"\n",
    "    num_correct = 0\n",
    "    for key, value in puzzle[\"solution\"].items():\n",
    "        if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "            match = matches[-1]\n",
    "            if match.strip().lower() == value.lower():\n",
    "                num_correct += 1\n",
    "    reward = acc = num_correct / len(puzzle[\"solution\"])\n",
    "    return art.Trajectory(\n",
    "        messages_and_choices=messages_and_choices,\n",
    "        reward=reward - hints_shared * 0.1,\n",
    "        metrics={\"acc\": acc, \"hints_shared\": hints_shared},\n",
    "    )\n",
    "\n",
    "\n",
    "val_groups, train_groups = await asyncio.gather(\n",
    "    art.gather_groups(\n",
    "        ((rollout(openai_client, puzzle) for _ in range(2)) for puzzle in val_puzzles),\n",
    "        pbar_desc=\"val\",\n",
    "    ),\n",
    "    art.gather_groups(\n",
    "        (\n",
    "            (rollout(openai_client, puzzle) for _ in range(50))\n",
    "            for puzzle in train_puzzles[i * stride : (i + 1) * stride]\n",
    "        ),\n",
    "        pbar_desc=\"train\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07499999999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
